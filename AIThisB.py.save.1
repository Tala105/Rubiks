import sys
import numpy as np
import tensorflow as tf
from time import sleep
from tf_agents.environments import py_environment, tf_py_environment, BatchedPyEnvironment
from tf_agents.specs import array_spec
from tf_agents.trajectories import time_step as ts, trajectory
from tf_agents.networks import q_network
from tf_agents.agents.dqn import dqn_agent
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.utils import common
from cube import Cube
from cubeRender import CubeRenderer

solved_state = Cube().get_state()
RENDER = "--render" in sys.argv  # keeps your sys.argv logic

class FakeRenderer:
    def render(self):
        pass

class CustomEnv(py_environment.PyEnvironment):
    renderer_count = 0

    def __init__(self, render: bool = False):
        super().__init__()
        self.cube = Cube()
        if render:
            cols, rows = 4, 3
            width, height = 1920 // cols, 1080 // rows
            col = CustomEnv.renderer_count % cols
            row = CustomEnv.renderer_count // cols
            window_pos = (col * width, row * height)
            self.renderer = CubeRenderer([self.cube], display=(width, height), window_pos=window_pos)
            CustomEnv.renderer_count += 1
        else:
            self.renderer = FakeRenderer()
        sleep(1)
        self.cube.scramble()
        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=11)
        self._observation_spec = array_spec.ArraySpec(shape=(len(self.cube.get_state()),), dtype=np.int32)
        self._episode_ended = False
        self._step_count = 0
        self._last_action = None

    @property
    def moves(self):
        return [
            self.cube.U, self.cube.Ud,
            self.cube.R, self.cube.Rd,
            self.cube.F, self.cube.Fd,
            self.cube.D, self.cube.Dd,
            self.cube.L, self.cube.Ld,
            self.cube.B, self.cube.Bd
        ]

    def action_spec(self):
        return self._action_spec

    def observation_spec(self):
        return self._observation_spec

    def _reset(self):
        self.cube.scramble()
        self._episode_ended = False
        self._step_count = 0
        self._last_action = None
        return ts.restart(np.array(self.cube.get_state(), dtype=np.int32))

    def _step(self, action):
        action = int(action)
        if self._episode_ended:
            return self.reset()
        self.moves[action]()
        self.renderer.render()

        reward = -0.1
        undo_pairs = {0:1, 1:0, 2:3, 3:2, 4:5, 5:4, 6:7, 7:6, 8:9, 9:8, 10:11, 11:10}
        if self._last_action is not None and undo_pairs[action] == self._last_action:
            reward -= 1.0

        state = self.cube.get_state()
        if not hasattr(self, "_rewarded_indices"):
            self._rewarded_indices = set()
        for i, (s, sol) in enumerate(zip(state, solved_state)):
            if s == sol and i not in self._rewarded_indices:
                reward += 0.5
                self._rewarded_indices.add(i)

        if state == solved_state:
            reward += 100.0
            print("="*25 + "Cube solved!" + "="*25)
            self._episode_ended = True
        elif self._step_count >= 250:
            self._episode_ended = True

        self._step_count += 1
        self._last_action = action
        state = np.array(state, dtype=np.int32)

        if self._episode_ended:
            return ts.termination(state, reward)
        else:
            return ts.transition(state, reward, discount=0.99)

# --- Create 12 independent environments ---
NUM_AGENTS = 12
envs = [CustomEnv(render=RENDER) for _ in range(NUM_AGENTS)]
batched_env = BatchedPyEnvironment(envs)
tf_env = tf_py_environment.TFPyEnvironment(batched_env)

# --- DQN agent setup ---
q_net = q_network.QNetwork(
    tf_env.observation_spec(),
    tf_env.action_spec(),
    fc_layer_params=(128, 128)
)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
agent = dqn_agent.DqnAgent(
    tf_env.time_step_spec(),
    tf_env.action_spec(),
    q_network=q_net,
    optimizer=optimizer,
    td_errors_loss_fn=common.element_wise_squared_loss,
    train_step_counter=tf.Variable(0)
)
agent.initialize()

# --- Replay buffer ---
replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,
    batch_size=tf_env.batch_size,
    max_length=100000
)

# --- Data collection ---
def collect_data(env, policy, steps_per_env=1):
    time_step = env.reset()
    for _ in range(steps_per_env):
        action_step = policy.action(time_step)
        next_time_step = env.step(action_step.action)
        traj = trajectory.from_transition(time_step, action_step, next_time_step)
        replay_buffer.add_batch(traj)
        time_step = next_time_step

# --- Training step ---
@tf.function
def train_step(experience):
    return agent.train(experience).loss

# --- Training loop ---
NUM_ITERATIONS = 1000
COLLECT_STEPS_PER_ITER = 10
BATCH_SIZE = 64
NUM_TRAIN_STEPS = 10

for iteration in range(NUM_ITERATIONS):
    collect_data(tf_env, agent.collect_policy, steps_per_env=COLLECT_STEPS_PER_ITER)
    for _ in range(NUM_TRAIN_STEPS):
        experience, _ = replay_buffer.get_next(sample_batch_size=BATCH_SIZE, num_steps=2)
        loss = train_step(experience)
    if iteration % 10 == 0:
        print(f"Iteration {iteration}: loss = {loss.numpy()}")
import numpy as np
import tensorflow as tf
import threading
import sys
from time import sleep
from tf_agents.environments import tf_py_environment, py_environment
from tf_agents.networks import q_network
from tf_agents.agents.dqn import dqn_agent
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.trajectories import trajectory
from tf_agents.utils import common
from tf_agents.specs import array_spec
from tf_agents.trajectories import time_step as ts
from cube import Cube
from cubeRender import CubeRenderer

solved_state = Cube().get_state()

class FakeRenderer: 
    def render(self): pass

class CustomEnv(py_environment.PyEnvironment):
    def __init__(self, render=False):
        super().__init__()
        self.cube = Cube()
        self.renderer = CubeRenderer([self.cube], display=(480,360), window_pos=(0,0)) if render else FakeRenderer()
        sleep(3); self.cube.scramble()
        self._action_spec = array_spec.BoundedArraySpec((), np.int32, 0, 11)
        self._observation_spec = array_spec.ArraySpec((len(self.cube.get_state()),), np.int32)
        self._episode_ended = False; self._step_count = 0; self._last_action = None

    @property
    def moves(self):
        return [self.cube.U,self.cube.Ud,self.cube.R,self.cube.Rd,self.cube.F,self.cube.Fd,
                self.cube.D,self.cube.Dd,self.cube.L,self.cube.Ld,self.cube.B,self.cube.Bd]

    def action_spec(self): return self._action_spec
    def observation_spec(self): return self._observation_spec

    def _reset(self):
        self.cube.scramble(); self._episode_ended=False; self._step_count=0; self._rewarded_indices=set()
        return ts.restart(np.array(self.cube.get_state(), np.int32))

    def _step(self, action):
        if self._episode_ended: return self.reset()
        self.moves[action](); self.renderer.render()
        reward=-0.1; undo={0:1,1:0,2:3,3:2,4:5,5:4,6:7,7:6,8:9,9:8,10:11,11:10}
        if self._last_action is not None and undo[action]==self._last_action: reward-=1.0
        state=self.cube.get_state()
        for i,(s,sol) in enumerate(zip(state,solved_state)):
            if s==sol and i not in self._rewarded_indices:
                reward+=0.5; self._rewarded_indices.add(i)
        if state==solved_state:
            reward+=100.0; print("="*25+" Cube solved! "+"="*25); self._episode_ended=True
        elif self._step_count>=250: self._episode_ended=True
        self._step_count+=1; self._last_action=action
        if self._episode_ended: return ts.termination(np.array(state,np.int32),reward)
        return ts.transition(np.array(state,np.int32),reward,discount=0.99)

tf_env=tf_py_environment.TFPyEnvironment(CustomEnv())
q_net=q_network.QNetwork(tf_env.observation_spec(),tf_env.action_spec(),fc_layer_params=(100,))
optimizer = tf.keras.optimizers.Adam(1e-3)
agent = dqn_agent.DqnAgent(tf_env.time_step_spec(), tf_env.action_spec(), q_network=q_net,
    optimizer=optimizer, td_errors_loss_fn=common.element_wise_squared_loss,
    train_step_counter=tf.Variable(0))

agent.initialize()

replay_buffer=tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,batch_size=tf_env.batch_size,max_length=100000)

NUM_AGENTS=12
if len(sys.argv) > 1:
    falsy = ["false", "0", "no", "n", "f"]
    rendering = sys.argv[1]
    if rendering in falsy:
        rendering = False
    else:
        rendering = True
    print(f"Rendering: {rendering}")

ENVS=[tf_py_environment.TFPyEnvironment(CustomEnv(render=rendering)) for _ in range(NUM_AGENTS)]

def collect_worker(env,policy,episodes=5):
    for _ in range(episodes):
        ts1=env.reset()
        while not ts1.is_last():
            act=policy.action(ts1)
            ts2=env.step(act.action)
            traj=trajectory.from_transition(ts1,act,ts2)
            replay_buffer.add_batch(traj)
            ts1=ts2

@tf.function
def train_step(exp): return agent.train(exp).loss

for i in range(1000):
    print(f"\n=== Iteration {i} ===")
    threads=[threading.Thread(target=collect_worker,args=(ENVS[j],agent.collect_policy,5)) for j in range(NUM_AGENTS)]
    [t.start() for t in threads]; [t.join() for t in threads]
    for _ in range(20):
        exp,_=replay_buffer.get_next(sample_batch_size=64,num_steps=2)
        loss=train_step(exp)
    print(f"Train loss: {loss.numpy():.4f}")

