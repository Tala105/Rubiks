import numpy as np import tensorflow as tf import threading import sys from time import sleep 
from tf_agents.environments import tf_py_environment, py_environment from tf_agents.networks 
import q_network from tf_agents.agents.dqn import dqn_agent from tf_agents.replay_buffers import 
tf_uniform_replay_buffer from tf_agents.trajectories import trajectory from tf_agents.utils 
import common from tf_agents.specs import array_spec from tf_agents.trajectories import time_step 
as ts from cube import Cube from cubeRender import CubeRenderer


solved_state = Cube().get_state()

class FakeRenderer: 
    def render(self): pass

class CustomEnv(py_environment.PyEnvironment):
    def __init__(self, render=False):
        super().__init__()
        self.cube = Cube()
        self.renderer = CubeRenderer([self.cube], display=(480,360), window_pos=(0,0)) if render else FakeRenderer()
        sleep(3); self.cube.scramble()
        self._action_spec = array_spec.BoundedArraySpec((), np.int32, 0, 11)
        self._observation_spec = array_spec.ArraySpec((len(self.cube.get_state()),), np.int32)
        self._episode_ended = False; self._step_count = 0; self._last_action = None

    @property
    def moves(self):
        return [self.cube.U,self.cube.Ud,self.cube.R,self.cube.Rd,self.cube.F,self.cube.Fd,
                self.cube.D,self.cube.Dd,self.cube.L,self.cube.Ld,self.cube.B,self.cube.Bd]

    def action_spec(self): return self._action_spec
    def observation_spec(self): return self._observation_spec

    def _reset(self):
        self.cube.scramble(); self._episode_ended=False; self._step_count=0; self._rewarded_indices=set()
        return ts.restart(np.array(self.cube.get_state(), np.int32))

    def _step(self, action):
        if self._episode_ended: return self.reset()
        self.moves[action](); self.renderer.render()
        reward=-0.1; undo={0:1,1:0,2:3,3:2,4:5,5:4,6:7,7:6,8:9,9:8,10:11,11:10}
        if self._last_action is not None and undo[action]==self._last_action: reward-=1.0
        state=self.cube.get_state()
        for i,(s,sol) in enumerate(zip(state,solved_state)):
            if s==sol and i not in self._rewarded_indices:
                reward+=0.5; self._rewarded_indices.add(i)
        if state==solved_state:
            reward+=100.0; print("="*25+" Cube solved! "+"="*25); self._episode_ended=True
        elif self._step_count>=250: self._episode_ended=True
        self._step_count+=1; self._last_action=action
        if self._episode_ended: return ts.termination(np.array(state,np.int32),reward)
        return ts.transition(np.array(state,np.int32),reward,discount=0.99)

tf_env=tf_py_environment.TFPyEnvironment(CustomEnv())
q_net=q_network.QNetwork(tf_env.observation_spec(),tf_env.action_spec(),fc_layer_params=(100,))
optimizer = tf.keras.optimizers.Adam(1e-3)
agent = dqn_agent.DqnAgent(tf_env.time_step_spec(), tf_env.action_spec(), q_network=q_net,
    optimizer=optimizer, td_errors_loss_fn=common.element_wise_squared_loss,
    train_step_counter=tf.Variable(0))

agent.initialize()

replay_buffer=tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,batch_size=tf_env.batch_size,max_length=100000)

NUM_AGENTS=12
if len(sys.argv) > 1:
    falsy = ["false", "0", "no", "n", "f"]
    rendering = sys.argv[1]
    if rendering in falsy:
        rendering = False
    else:
        rendering = True
    print(f"Rendering: {rendering}")

ENVS=[tf_py_environment.TFPyEnvironment(CustomEnv(render=rendering)) for _ in range(NUM_AGENTS)]

def collect_worker(env,policy,episodes=5):
    for _ in range(episodes):
        ts1=env.reset()
        while not ts1.is_last():
            act=policy.action(ts1)
            ts2=env.step(act.action)
            traj=trajectory.from_transition(ts1,act,ts2)
            replay_buffer.add_batch(traj)
            ts1=ts2

@tf.function
def train_step(exp): return agent.train(exp).loss

for i in range(1000):
    print(f"\n=== Iteration {i} ===")
    threads=[threading.Thread(target=collect_worker,args=(ENVS[j],agent.collect_policy,5)) for j in range(NUM_AGENTS)]
    [t.start() for t in threads]; [t.join() for t in threads]
    for _ in range(20):
        exp,_=replay_buffer.get_next(sample_batch_size=64,num_steps=2)
        loss=train_step(exp)
    print(f"Train loss: {loss.numpy():.4f}")

